{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, csv, subprocess, sys, re\n",
    "from urllib.request import urlopen\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from git import *\n",
    "from subprocess import Popen, PIPE\n",
    "from os import path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"\"        # put the cloned repository name in the project, e.g. activemq, hbase, etc.\n",
    "bugidentifier = \"\"  # put the bug identifier from the project, e.g. AMQ for activemq project, HBASE for hbase project, etc.\n",
    "userhome = os.path.expanduser('~')\n",
    "repository = userhome + r'/DifferentDiffAlgorithms/SZZ/datasource/' + project + '/'\n",
    "analyze_dir = userhome + r'/DifferentDiffAlgorithms/SZZ/projects_analyses/' + project + '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining git command function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_command(cmd, work_dir):\n",
    "    pipe = subprocess.Popen(cmd, shell=True, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (out, error) = pipe.communicate()\n",
    "    return out, error\n",
    "    pipe.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding log messages containing bug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['bug', 'fix', 'defect', 'patch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying git command to find commit log messages containing words of bugs, fix, defect or patch\n",
    "logs = []\n",
    "for n, word in enumerate(words):\n",
    "    git_cmd = \"git log --all --grep='\" + word + \"' --oneline\"\n",
    "    log = (str(execute_command(git_cmd, repository)).replace(\"b'\",'').replace('b\"','').replace('(','',1).replace(\"\\\\'\",\"'\").split(\"\\\\n\"))[:-1]\n",
    "    logs.append(log)\n",
    "\n",
    "com_logs = [item for sublist in logs for item in sublist]\n",
    "com_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the commit id and the log messages\n",
    "commit = []\n",
    "for xx in range(0,len(com_logs)):\n",
    "    tmp = []\n",
    "    comm = com_logs[xx].split()\n",
    "    word = ' '.join(comm[1:])\n",
    "    tmp.extend([comm[0],word])\n",
    "    commit.append(tmp)\n",
    "\n",
    "for item in commit:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capturing candidate bug-fix commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugfix_commit = []\n",
    "for t in range(0,len(commit)):\n",
    "    com = commit[t][0]\n",
    "    try:\n",
    "        bugid = re.search(\"(\" + bugidentifier + \"-[0-9]+)\", commit[t][1])\n",
    "        bugid = bugid.groups()[0]\n",
    "        bugfix_commit.append([com, bugid])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print (\"Number of commit logs = %i\" % len(bugfix_commit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Capturing only bug id candidates which are identified as bug links in the log messages \n",
    "# to synchronize the type of bug id from JIRA ITS\n",
    "commitbugs = []\n",
    "buglinks = []\n",
    "\n",
    "for xx in range(0,len(bugfix_commit)):\n",
    "    if bugfix_commit[xx][1] not in buglinks:\n",
    "        buglinks.append(bugfix_commit[xx][1])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "buglinks = sorted(buglinks, reverse=True)\n",
    "print (\"\\nNumber of bug id: \" + str(len(buglinks)))\n",
    "buglinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronizing the bug ids with the bug report from JIRA ITS database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synchronizing the bug ids with the bug report from JIRA ITS database\n",
    "errorlinks = []\n",
    "id_type = []\n",
    "for a,b in enumerate(buglinks):\n",
    "    link = \"https://issues.apache.org/jira/browse/\" + b\n",
    "    sys.stdout.write(\"\\r%i \" %(a+1) + \"Extracting: \" + b)\n",
    "    sys.stdout.flush()\n",
    "    try:\n",
    "        page = urllib.request.urlopen(link)\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        resolution = soup.find('span', attrs={'id':'resolution-val'}).text.replace(\"\\n\",'').replace(\" \",'').split(\",\")\n",
    "        types = soup.find('span', attrs={'id':'type-val'}).text.replace(\"\\n\",'').replace(\" \",'').split(\",\")\n",
    "        types = sorted(types)\n",
    "        types.insert(0, b)\n",
    "        types.insert(2,resolution[0])\n",
    "        id_type.append(types)\n",
    "    except:\n",
    "        errorlinks.append(b)\n",
    "\n",
    "print(\"\\nExtraction has been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding out if there were any uncollected type of bug ids from the sync process\n",
    "type_of_id = id_type\n",
    "errorlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resync for the error bug ids identified\n",
    "#This program should be run multiple times if there are still some unscrapped bug ids due to network problems\n",
    "error_links = []\n",
    "for a,b in enumerate(errorlinks):\n",
    "    link = \"https://issues.apache.org/jira/browse/\" + b\n",
    "    sys.stdout.write(\"\\r%i \" %(a+1) + \"Extracting: \" + b)\n",
    "    sys.stdout.flush()\n",
    "    try:\n",
    "        page = urllib.request.urlopen(link)\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        resolution = soup.find('span', attrs={'id':'resolution-val'}).text.replace(\"\\n\",'').replace(\" \",'').split(\",\")\n",
    "        types = soup.find('span', attrs={'id':'type-val'}).text.replace(\"\\n\",'').replace(\" \",'').split(\",\")\n",
    "        types = sorted(types)\n",
    "        types.insert(0, b)\n",
    "        types.insert(2,resolution[0])\n",
    "        id_type.append(types)\n",
    "    except:\n",
    "        error_links.append(b)\n",
    "\n",
    "print(\"\\nExtraction has been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_id.sort()\n",
    "error_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bug_links = []\n",
    "for d, types in enumerate (type_of_id):\n",
    "    if types[1] == 'Bug' and types[2] == 'Fixed':\n",
    "        bug_links.append(types[0])\n",
    "\n",
    "print (\"Number of bug id: \" + str(len(bug_links)))\n",
    "print (bug_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (analyze_dir + \"01_bug_ids_extraction/candidate_bug_ids.txt\", mode=\"wt\", encoding=\"utf-8\") as myfile:\n",
    "    myfile.write('\\n'.join(bug_links))\n",
    "print (\"File candidate_bug_links.txt has been created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting parent_id of bug-fix commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_id = []\n",
    "for v, w in enumerate(bugfix_commit):\n",
    "    sys.stdout.write(\"\\rExtracting parent id from bug-fix commit: {} / {}\".format((v+1), len(bugfix_commit)))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if w[1] in bug_links:\n",
    "        git_cmd = \"git log --pretty=%P -n1 \" + w[0]\n",
    "        temp = (str(execute_command(git_cmd, repository)).replace(\"b'\",'').replace('(','',1).split(\"\\\\n\"))[:-1]\n",
    "        dt = [w[1],w[0],temp[0]]\n",
    "        parent_id.append(dt)\n",
    "print (\"\\nFinished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parent_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving parent id in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(analyze_dir + \"01_bug_ids_extraction/parentid_of_bugfixcommit.csv\", 'w') as parent:\n",
    "    header = ['bug_id','bugfix_commitID','parent_id']\n",
    "    writers = csv.writer(parent, delimiter=',')\n",
    "    writers.writerow(header)\n",
    "    for item in parent_id:\n",
    "        writers.writerow(item)\n",
    "\n",
    "dfparent = pd.read_csv(analyze_dir + \"01_bug_ids_extraction/parentid_of_bugfixcommit.csv\")\n",
    "dfparent = dfparent[header]\n",
    "dfparent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving commit_id of candidate bug fix commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commitid_fix = dfparent[['bug_id','bugfix_commitID']]\n",
    "commitid_fix.to_csv(analyze_dir + \"01_bug_ids_extraction/candidatebugfix_commitid.csv\", index=False)\n",
    "print (\"File commitid_of_candidatebugfix.csv has been created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
