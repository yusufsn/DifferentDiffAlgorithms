{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, subprocess, operator\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining repository and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"activemq\"      # put the cloned repository name in the project\n",
    "userhome = os.path.expanduser('~')\n",
    "repository = userhome + r'/DifferentDiffAlgorithms/SZZ/datasource/' + project + '/'\n",
    "analyze_dir = userhome + r'/DifferentDiffAlgorithms/SZZ/projects_analyses/' + project + '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load textfile contains bug-ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = open(analyze_dir + \"01_bug_ids_extraction/candidate_bug_ids.txt\", \"r\")\n",
    "bug_links = txt_file.read().split('\\n')\n",
    "print (\"Found \" + str(len(bug_links)) + \" bug_ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding affected versions by bug ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_links = []\n",
    "affected_version = []\n",
    "for a,b in enumerate(bug_links):\n",
    "    link = \"https://issues.apache.org/jira/browse/\" + b\n",
    "    sys.stdout.write(\"\\r%i \" %(a+1) + \"Extracting: \" + b)\n",
    "    sys.stdout.flush()\n",
    "    try:\n",
    "        page = urllib.request.urlopen(link)\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        aff_version = soup.find('span', attrs={'id':'versions-val'}).text.replace(\"\\n\",'').replace(\" M\",'-M').replace(\" \",'').replace(\".x\",'.').split(\",\")\n",
    "        aff_version = sorted(aff_version)\n",
    "        aff_version.insert(0,b)\n",
    "        affected_version.append(aff_version)\n",
    "    except:\n",
    "        error_links.append(b)\n",
    "\n",
    "print(\"\\nExtraction has been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (error_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat the process if there are still some affected versions by bug_ids haven't been captured due to network problems\n",
    "errorlinks = []\n",
    "if error_links != []:\n",
    "    for c,d in enumerate(error_links):\n",
    "        link = \"https://issues.apache.org/jira/browse/\" + d\n",
    "        sys.stdout.write(\"\\r%i \" %(c+1) + \"Extracting: \" + d)\n",
    "        sys.stdout.flush()\n",
    "        try:\n",
    "            page = urllib.request.urlopen(link)\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            types = soup.find('span', attrs={'id':'versions-val'}).text.replace(\"\\n\",'').replace(\" M\",'-M').replace(\" \",'').replace(\".x\",'.').split(\",\")\n",
    "            types = sorted(types)\n",
    "            types.insert(0, d)\n",
    "            affected_version.append(types)\n",
    "        except:\n",
    "            errorlinks.append(d)\n",
    "\n",
    "print (\"\\nExtraction is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (errorlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "affected_version.sort()\n",
    "\n",
    "#Finding the earliest version affected by the bug ids\n",
    "earliest_version = []\n",
    "for num, affver in enumerate(affected_version):\n",
    "    earliest_version.append(affver[:2])\n",
    "\n",
    "earliest_version.sort()\n",
    "for early in earliest_version:\n",
    "    print (early)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the function for git command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_command(cmd, work_dir):\n",
    "    #Executes a shell command in a subprocess, waiting until it has completed.\n",
    "    pipe = subprocess.Popen(cmd, shell=True, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (out, error) = pipe.communicate()\n",
    "    return out, error\n",
    "    pipe.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the versions related with earliest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "related_version = []\n",
    "for n, item in enumerate(earliest_version):\n",
    "    if \".\" in item[1]:\n",
    "        git_cmd = 'git tag -l \"*' + item[1] + '*\"'\n",
    "        temp = str(execute_command(git_cmd, repository)).replace(\"b'\",'').replace(\"(\",'').replace(\")\",'').split(\"\\\\n\")\n",
    "        del temp[len(temp)-1]\n",
    "        if temp == []:\n",
    "            temp = [item[1]]\n",
    "    else:\n",
    "        temp = ['None']\n",
    "\n",
    "    temp.insert(0, item[0])\n",
    "    related_version.append(temp)\n",
    "\n",
    "for xx in related_version:\n",
    "    print (xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the date release for affected version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_release = []\n",
    "for n, item in enumerate(related_version):\n",
    "    sys.stdout.write(\"\\rFinding datetime for version {}: {}\".format(n+1, item[0]))\n",
    "    sys.stdout.flush()\n",
    "    if item[1] != \"None\":\n",
    "        for m in range(1, len(item)):\n",
    "            git_cmd = \"git log -1 --format=%ai \" + item[m]\n",
    "            temp = str(execute_command(git_cmd, repository)).replace(\"b'\",'').replace(\"(\",'').replace(\")\",'').split(\"\\\\n\")\n",
    "            del temp[len(temp)-1]\n",
    "            temp = temp[0].split(\" \")\n",
    "            if temp[0] != \"',\":\n",
    "                temp.insert(0,item[0])\n",
    "                temp.insert(1,item[m])\n",
    "                date_release.append(temp)\n",
    "                date_release = sorted(date_release, key=operator.itemgetter(0, 2))\n",
    "    \"\"\"else:\n",
    "        date_release.append(item)\"\"\"\n",
    "\n",
    "date_release = sorted(date_release, key=operator.itemgetter(0), reverse=True) \n",
    "\n",
    "# print(\"\\n\")\n",
    "# for jj in date_release:\n",
    "#     print (jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in CSV file\n",
    "with open(analyze_dir + '04_affected_versions/affected_version.csv','w') as csvfile:\n",
    "    writers = csv.writer(csvfile)\n",
    "    writers.writerow(['bug_id','earliest_affected_version','date_release','time_release','tz'])\n",
    "    for item in date_release:\n",
    "        writers.writerow(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(analyze_dir + '04_affected_versions/affected_version.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earliest_vers = df.groupby('bug_id', as_index=False).first()\n",
    "earliest_vers = earliest_vers.sort_values(['date_release', 'time_release', 'earliest_affected_version'], ascending=True)\n",
    "earliest_vers.to_csv(analyze_dir + '04_affected_versions/earliest_version.csv', index=False)\n",
    "earliest_vers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining 2 csv files: list of annotated files and earliest affected versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colname = ['bug_id','bugfix_commitID','parent_id','filepath','diff_myers_file','diff_histogram_file','blame_myers_file','blame_histogram_file',\n",
    "           '#deletions_myers','#deletions_histogram']\n",
    "filedata = pd.read_csv(analyze_dir + '03_annotate/01_annotated_files/listof_diff_n_annotated_files/diff_n_blame_combination_files.csv')\n",
    "filedata = filedata[colname]\n",
    "\n",
    "details = filedata.join(earliest_vers.set_index('bug_id')[['earliest_affected_version','date_release']], on='bug_id')\n",
    "details.to_csv(analyze_dir + '04_affected_versions/affected_version_for_identified_files.csv', index=False)\n",
    "\n",
    "print (\"Affected version for identified files has been created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
